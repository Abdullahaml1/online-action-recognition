{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DataSet.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "MlfQPaSThHve",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "outputId": "d6bcdcb5-b851-45aa-d6c4-f45c530c1e28"
      },
      "cell_type": "code",
      "source": [
        "!df ~ --block-size=G"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Filesystem     1G-blocks  Used Available Use% Mounted on\n",
            "overlay             359G    8G      333G   3% /\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uQpNa20VlFbJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "1fc2ae90-dcee-4243-f9ca-a134e7f8dc6c"
      },
      "cell_type": "code",
      "source": [
        "%%sh\n",
        "# at /content/two-stream-action-recognition\n",
        "wget http://ftp.tugraz.at/pub/feichtenhofer/tsfusion/data/ucf101_jpegs_256.zip.001 &\n",
        "wget http://ftp.tugraz.at/pub/feichtenhofer/tsfusion/data/ucf101_jpegs_256.zip.002 &\n",
        "wget http://ftp.tugraz.at/pub/feichtenhofer/tsfusion/data/ucf101_jpegs_256.zip.003 &"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "lu8XONDjlMAg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "%%sh\n",
        "# at /content/two-stream-action-recognition\n",
        "cat ucf101_jpegs_256.zip.00* > ucf101_jpegs_256.zip &\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dRJfQZRLlfSg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# at /content/two-stream-action-recognition\n",
        "!rm ucf101_jpegs_256.zip.00*"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1IdrcCghlhSR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%sh\n",
        "# at /content/two-stream-action-recognition\n",
        "unzip ucf101_jpegs_256.zip >> zip1.out &\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ovp_o_POljFA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm ucf101_jpegs_256.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZPFjKfu1llkI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        },
        "outputId": "55459dfc-c23b-4672-8aff-6fa6839ef854"
      },
      "cell_type": "code",
      "source": [
        "!du -sh --human-readable *"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.0K\tclassInd.txt\n",
            "33G\tjpegs_256\n",
            "55M\tsample_data\n",
            "144K\ttestlist01.txt\n",
            "388K\ttrainlist01.txt\n",
            "149M\tzip1.out\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "x2Uu1kb04TE5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "outputId": "f0344481-b296-4bf6-f30a-30b258391bed"
      },
      "cell_type": "code",
      "source": [
        "!pip3 install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==0.3.0.post4 from http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl\n",
            "\u001b[?25l  Downloading http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl (592.3MB)\n",
            "\u001b[K    100% |████████████████████████████████| 592.3MB 47.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==0.3.0.post4) (1.14.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from torch==0.3.0.post4) (3.13)\n",
            "Installing collected packages: torch\n",
            "Successfully installed torch-0.3.0.post4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "l1HF46wl5kSy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "897f9369-8a38-4540-b6b4-2093a8fe3181"
      },
      "cell_type": "code",
      "source": [
        "!pip3 install torchvision"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchvision\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n",
            "\r\u001b[K    18% |██████                          | 10kB 18.4MB/s eta 0:00:01\r\u001b[K    37% |████████████                    | 20kB 1.8MB/s eta 0:00:01\r\u001b[K    56% |██████████████████              | 30kB 2.6MB/s eta 0:00:01\r\u001b[K    75% |████████████████████████        | 40kB 1.7MB/s eta 0:00:01\r\u001b[K    93% |██████████████████████████████  | 51kB 2.1MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 61kB 2.4MB/s \n",
            "\u001b[?25hCollecting pillow>=4.1.1 (from torchvision)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/94/5430ebaa83f91cc7a9f687ff5238e26164a779cca2ef9903232268b0a318/Pillow-5.3.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 14.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision) (0.3.0.post4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from torch->torchvision) (3.13)\n",
            "Installing collected packages: pillow, torchvision\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-5.3.0 torchvision-0.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WzTs2RAw4LvY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.utils.data as data\n",
        "\n",
        "from PIL import Image\n",
        "import os\n",
        "import os.path\n",
        "import numpy as np\n",
        "from numpy.random import randint"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nWWhKtha3ciQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class VidInfo(object):\n",
        "  '''\n",
        "  row: Vidoes info imported form file list [Directory,# of frames,label]\n",
        "  '''\n",
        "    def __init__(self, row):\n",
        "        self.data = row\n",
        "\n",
        "    @property  \n",
        "    #property: Insted of writing obj.path(), you can write obj.path .It conceals the intial value of row/data inside the fuction and can't be changed \n",
        "    #even if data is overwrited later\n",
        "    def path(self):\n",
        "        return self.data[0]\n",
        "\n",
        "    @property\n",
        "    def num_frames(self):\n",
        "        return int(self.data[1])\n",
        "\n",
        "    @property\n",
        "    def label(self):\n",
        "        return int(self.data[2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "saWejVJYznNf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class TSNdataset(data.Dataset):\n",
        "  '''\n",
        "  file_list         : FileList.txt \n",
        "  num_segments      : Default = 3\n",
        "  new_length        : The num of sequentially picked from the frames (1 'RGB' , 2 with 'RGBDiff')\n",
        "  modality          : Takes two values 'RGB'&'RGBDiff'\n",
        "  image_prefix      : Prefix of the frames name\n",
        "  transform         : Pytorch composion of transformations (Data Augmentation)\n",
        "  train_test_switch : True for train ,False for test\n",
        "  \n",
        "  The overall use of this class is :\n",
        "  >>TestObj = TSNdataset(......)                    #Creating an object\n",
        "  >>TestObj[Index]                                  #Index : is the index for a video in the dataset (0:13321)\n",
        "  \n",
        "  previous line gives a tuple (List of the picked frames,The label for this video)\n",
        "  \n",
        "  \n",
        "  '''\n",
        "  def __init__(self, root_path, file_list,num_segments=3, new_length=1, modality='RGB',\n",
        "                 image_prefix='frame{:06d}.jpg', transform=None,test_mode=False ,train_test_switch=True):\n",
        "    \n",
        "    self.root_path = root_path\n",
        "    self.file_list = file_list\n",
        "    self.num_segments = num_segments\n",
        "    self.new_length = new_length\n",
        "    self.modality = modality\n",
        "    self.image_prefix = image_prefix\n",
        "    self.transform = transform\n",
        "    self.test_mode = test_mode\n",
        "    \n",
        "    if self.modality == 'RGBDiff' :\n",
        "      new_length += 1                                           \n",
        "    \n",
        "    #List of Objects, Everyone of them corresponding to a video\n",
        "    self.VidInfoList = [VidInfo(x.strip().split('  '))  for x in self.file_list]               \n",
        "  \n",
        "  \n",
        "  def Load_Img (self,directory,idx):\n",
        "    '''\n",
        "    directory : Video path\n",
        "    idx : Frames suffix (Frame number eg. 000013)\n",
        "    '''\n",
        "    return [Image.open(os.path.join(directory,self.image_prefix.format(idx))).convert('RGB')]\n",
        "  \n",
        "  \n",
        "  \n",
        "  def Train_Sample_indices(self,vid_info):\n",
        "    '''\n",
        "    vid_info : Object has info about one video\n",
        "    This function determine the indices for the chosen frames\n",
        "    '''\n",
        "    if vid_info.num_frames < self.num_segments:\n",
        "      return np.array(range(vid_info.num_frames))+1                                             #Return all frames inside short video\n",
        "    \n",
        "    FPSeg = vid_info.num_frames // self.num_segments                                            #Frames per segment\n",
        "    offest = [x*FPSeg for x in range(num_segments)]                                             #The first index for every segment \n",
        "    frame_indices = list(randint(FPSeg,size=self.num_segments))                                 #Chosen frames form every segment\n",
        "    sample_indices = [sum(i) for i in zip(frame_indices,offest)]                                #elment-wise sumtion of offest and smaple_indices\n",
        "    return np.array(sample_indices)+1                                                           #Frames indices starts from 000001\n",
        "  \n",
        "  \n",
        "  \n",
        "  def Val_Sample_indices(self,vid_info):\n",
        "    '''\n",
        "    vid_info : Object has info about one video\n",
        "    This function determine the indices for the chosen frames\n",
        "    '''\n",
        "    if vid_info.num_frames < self.num_segments:\n",
        "      return np.array(range(vid_info.num_frames))+1\n",
        "    \n",
        "    FPSeg = vid_info.num_frames / float(self.num_segments)\n",
        "    sample_indices = [int(FPSeg*( x + 1/2.0 )) for x in range(self.num_segments)]               #Get the middle frame for every segment\n",
        "    return np.array(sample_indices)+1\n",
        "  \n",
        "  \n",
        "  \n",
        "  def Test_Sample_indices(self,vid_info):\n",
        "    '''\n",
        "    vid_info : Object has info about one video\n",
        "    This function determine the indices for the chosen frames\n",
        "    '''    \n",
        "    if vid_info.num_frames < self.num_segments:\n",
        "      return np.array(range(vid_info.num_frames))+1\n",
        "    \n",
        "    FPSeg = vid_info.num_frames / float(self.num_segments)\n",
        "    sample_indices = [int(FPSeg*( x + 1/2.0 )) for x in range(self.num_segments)]\n",
        "    return np.array(sample_indices)+1\n",
        "    \n",
        "    \n",
        "    \n",
        "  def Vid2Frames(self,vid_info,indices):\n",
        "    '''\n",
        "    vid_info : Object has info about one video\n",
        "    indices  : Indices for the chosen frames\n",
        "    '''\n",
        "    images = []\n",
        "    for idx in indices:                                                         #For every frame in the chosen frames\n",
        "      Img = self.Load_Img (vid_info.path,idx)                                   #Load the frame\n",
        "      images.extend(Img)                                                        #Stack it with the others\n",
        "      \n",
        "    processed_frames = self.transform(images)                                   #Applay transform stuff on the chosen frames\n",
        "    return processed_frames, vid_info.label                                       \n",
        "  \n",
        "  \n",
        "  \n",
        "  def __getitem__(self,idx):\n",
        "    '''\n",
        "    idx : Index for a video into the dataset\n",
        "    The main perpuse of this function is to interact 'nicer' with the object of the class\n",
        "    >>TestObj[3] \n",
        "    This line output a tuple (List of chosen frames,label) for the 3rd video in the dataset\n",
        "    '''\n",
        "    vid_info = self.VidInfoList[idx]\n",
        "    if test_mode :\n",
        "      indices = self.Test_Sample_indices(vid_info)\n",
        "    else:\n",
        "      indices = self.Trian_Sample_indices(vid_info) if train_test_swtich else self.Val_Sample_indices(vid_info)  \n",
        "    return self.Vid2Frames(vid_info,indices)\n",
        "    \n",
        "  \n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}